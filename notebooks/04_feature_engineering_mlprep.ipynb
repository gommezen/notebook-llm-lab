{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# ðŸ“˜ Notebook: 04_feature_engineering_mlprep.ipynb\n",
    "#\n",
    "# Purpose:\n",
    "#   Derive secondary metrics from cleaned run summaries and prepare\n",
    "#   a machine-learningâ€“ready dataset for clustering and regression.\n",
    "#\n",
    "# Input : data/strava/processed/run_summary_cleaned.parquet\n",
    "# Output: data/strava/processed/runs_features.parquet\n",
    "# Next  : Stage 5 â€“ Clustering & Explainability\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# --- 4.1. Load cleaned dataset -------------------------------------------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = Path(\"../data/strava/processed/run_summary_cleaned.parquet\")\n",
    "\n",
    "df = pd.read_parquet(data_path)\n",
    "print(f\"âœ… Loaded {len(df):,} cleaned runs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2 Derived feature engineering -----------------------------------------\n",
    "# Rolling variability of pace (how consistent you were across runs)\n",
    "df[\"pace_variability\"] = df[\"avg_pace\"].rolling(5, min_periods=1).std()\n",
    "\n",
    "# Fatigue index: higher if you climb more or slow down with elevation\n",
    "df[\"fatigue_index\"] = (df[\"avg_pace\"] * (df[\"elevation_gain\"] + 1)) / (df[\"avg_cadence\"] + 1)\n",
    "\n",
    "# Elevation per km: terrain load indicator\n",
    "df[\"elev_ratio\"] = df[\"elevation_gain\"] / (df[\"total_distance_km\"] + 1e-3)\n",
    "\n",
    "# Day-of-week categorical\n",
    "df[\"weekday\"] = pd.to_datetime(df[\"date\"]).dt.day_name()\n",
    "\n",
    "# Quick preview\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3. Normalization / scaling ---------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaled_cols = [\"avg_pace\", \"avg_cadence\", \"elevation_gain\", \"fatigue_index\"]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_scaled = df.copy()\n",
    "df_scaled[scaled_cols] = scaler.fit_transform(df_scaled[scaled_cols])\n",
    "\n",
    "print(\"âœ… Normalized key metrics for modeling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.4. Correlation and sanity plots ----------------------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(df_scaled[scaled_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.5. Optional dimensionality preview (PCA) -------------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Drop any rows that have NaN in the scaled columns\n",
    "#df_pca = df_scaled.dropna(subset=scaled_cols)\n",
    "\n",
    "#--------\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Safety: ensure df_scaled exists. If not, build it from df (so this cell is runnable standalone).\n",
    "if 'df_scaled' not in globals():\n",
    "    if 'df' in globals():\n",
    "        print(\"df_scaled not defined â€” creating df_scaled from df (scaling selected cols).\")\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = df.copy()\n",
    "        # Fill NaNs with column mean before scaling\n",
    "        df_scaled[scaled_cols] = df_scaled[scaled_cols].fillna(df_scaled[scaled_cols].mean())\n",
    "        df_scaled[scaled_cols] = scaler.fit_transform(df_scaled[scaled_cols])\n",
    "    else:\n",
    "        raise NameError(\"Neither df_scaled nor df are defined. Run the earlier cells to load the dataset.\")\n",
    "\n",
    "# Prepare the matrix for PCA\n",
    "X = df_scaled[scaled_cols].values\n",
    "# Impute any remaining missing values (safety)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "#------------------\n",
    "pca = PCA(n_components=2)\n",
    "# proj = pca.fit_transform(df_pca[scaled_cols])\n",
    "\n",
    "# plt.figure(figsize=(7,5))\n",
    "# plt.scatter(\n",
    "#     proj[:, 0],\n",
    "#     proj[:, 1],\n",
    "#     c=df_pca[\"avg_pace\"],\n",
    "#     cmap=\"viridis\",\n",
    "#     s=40,\n",
    "#     alpha=0.7\n",
    "# )\n",
    "# plt.title(\"PCA projection (color = avg_pace)\")\n",
    "# plt.xlabel(\"PC1\")\n",
    "# plt.ylabel(\"PC2\")\n",
    "# plt.show()\n",
    "\n",
    "#-----\n",
    "proj = pca.fit_transform(X_imputed)\n",
    "\n",
    "# Report explained variance\n",
    "explained = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance: PC1={explained[0]:.2%}, PC2={explained[1]:.2%}, total={(explained[:2].sum()):.2%}\")\n",
    "\n",
    "# Plot the projection. Color points by avg_pace to indicate a continuous label\n",
    "plt.figure(figsize=(8,6))\n",
    "sc = plt.scatter(proj[:,0], proj[:,1], c=df_scaled[\"avg_pace\"], cmap=\"viridis\", s=40, alpha=0.8)\n",
    "plt.colorbar(sc, label='avg_pace')\n",
    "\n",
    "# Overlay feature loadings as arrows to show which features push points in each direction\n",
    "loadings = pca.components_.T\n",
    "x_load = loadings[:,0]\n",
    "y_load = loadings[:,1]\n",
    "feature_names = scaled_cols\n",
    "\n",
    "# Scale loadings for visibility (arbitrary scaling factor)\n",
    "scale = np.max(np.abs(proj)) * 0.6 if proj.size else 1.0\n",
    "for i, (x_l, y_l) in enumerate(zip(x_load, y_load)):\n",
    "    plt.arrow(0, 0, x_l*scale, y_l*scale, color='red', alpha=0.7, head_width=0.03)\n",
    "    plt.text(x_l*scale*1.15, y_l*scale*1.15, feature_names[i], color='red', fontsize=9)\n",
    "\n",
    "plt.title('PCA projection (color = avg_pace)')\n",
    "plt.xlabel(f\"PC1 ({explained[0]*100:.1f}% var)\")\n",
    "plt.ylabel(f\"PC2 ({explained[1]*100:.1f}% var)\")\n",
    "plt.axhline(0, color='grey', lw=0.5, linestyle='--')\n",
    "plt.axvline(0, color='grey', lw=0.5, linestyle='--')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Interpretation tips:\n",
    "# - Points close together in this plot have similar values across the input features.\n",
    "# - The color indicates avg_pace; clustering by color shows pace-driven structure.\n",
    "# - Feature loadings (red arrows) indicate the direction each standardized feature\n",
    "#   increases in the PCA space. For example, an arrow pointing to the right means\n",
    "#   that higher values of that feature push samples toward larger PC1 values.\n",
    "# - Check the explained variance above to judge whether 2 components are sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "loadings = pca.components_.T  # shape: (n_features, n_components)\n",
    "pc1_loadings = pd.Series(loadings[:,0], index=scaled_cols).abs().sort_values(ascending=False)\n",
    "pc2_loadings = pd.Series(loadings[:,1], index=scaled_cols).abs().sort_values(ascending=False)\n",
    "print('Top contributors to PC1:\\n', pc1_loadings)\n",
    "print('Top contributors to PC2:\\n', pc2_loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.6. Export ML-ready features --------------------------------------------\n",
    "out_path = Path(\"data/strava/processed/runs_features.parquet\")\n",
    "df_scaled.to_parquet(out_path, index=False)\n",
    "print(f\"âœ… ML-ready feature dataset saved â†’ {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.7.. Inspect columns before feature engineering -------------------------\n",
    "df.columns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#4.8. Derived Features Examples (all safe to compute now):\n",
    "\n",
    "df[\"pace_variability\"] = df[\"avg_pace\"].rolling(5, min_periods=1).std()\n",
    "df[\"fatigue_index\"] = (df[\"avg_pace\"] * df[\"elevation_gain\"]) / df[\"avg_cadence\"]\n",
    "df[\"elev_ratio\"] = df[\"elevation_gain\"] / df[\"total_distance_km\"]\n",
    "df[\"weekday\"] = pd.to_datetime(df[\"date\"]).dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "weekly = df.groupby(df[\"date\"].dt.isocalendar().week).agg(\n",
    "    weekly_distance=(\"total_distance_km\",\"sum\"),\n",
    "    avg_pace=(\"avg_pace\",\"mean\"),\n",
    "    fatigue_index=(\"fatigue_index\",\"mean\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "4.9. #Normalization / Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_cols = [\"avg_pace\",\"avg_cadence\",\"elevation_gain\",\"fatigue_index\"]\n",
    "df_scaled = df.copy()\n",
    "df_scaled[scaled_cols] = scaler.fit_transform(df_scaled[scaled_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#5.0 Correlation & Sanity Plots\n",
    "#Use seaborn or matplotlib:\n",
    "\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "sns.heatmap(df_scaled[scaled_cols].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 5. Optional dimensionality preview (PCA) -------------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Handle missing values safely\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(df_scaled[scaled_cols])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "proj = pca.fit_transform(X_imputed)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(\n",
    "    proj[:, 0],\n",
    "    proj[:, 1],\n",
    "    c=df_scaled[\"avg_pace\"],\n",
    "    cmap=\"viridis\",\n",
    "    s=40,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"PCA projection (color = avg_pace)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 6. Export ML-ready feature dataset --------------------------------------\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Detect the correct project root (so you donâ€™t get a nested /notebooks/data/ folder)\n",
    "cwd = Path(os.getcwd())\n",
    "project_root = cwd.parents[0] if cwd.name == \"notebooks\" else cwd\n",
    "\n",
    "# Define canonical processed output path\n",
    "processed_path = project_root / \"data\" / \"strava\" / \"processed\" / \"run_summary_features.parquet\"\n",
    "\n",
    "# Ensure directory exists\n",
    "processed_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the scaled + engineered dataset\n",
    "df_scaled.to_parquet(processed_path, index=False)\n",
    "\n",
    "print(f\"âœ… ML-ready feature dataset saved â†’ {processed_path.resolve()}\")\n",
    "print(f\"Columns exported: {len(df_scaled.columns)} â†’ {df_scaled.columns.tolist()}\")\n",
    "print(f\"Rows exported: {len(df_scaled):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-llm-lab",
   "language": "python",
   "name": "notebook-llm-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
