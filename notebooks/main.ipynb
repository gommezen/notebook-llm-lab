{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a172f73",
   "metadata": {},
   "source": [
    "# Building LLM prompts with local LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb1893e1-ad8b-4a30-b90e-2ab302861978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook demonstrates how to construct LLM prompts and interact with \n",
    "#a locally-hosted or packaged language model via the ask_model wrapper in llm_client.py.\n",
    "#Basic prompt examples (summarization, simplification, idea extraction).\n",
    "#Saving model outputs to outputs/results for reuse across notebooks.\n",
    "#Quick post-processing with TextBlob and NLTK to extract sentiment, noun phrases, and simple keyword signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1072ee09-f8b8-437f-9340-27aeaa9e4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.imports and parth setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de643429-b9a6-48dd-b15d-afce030a759a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ITSMARTSOLUTIONS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ITSMARTSOLUTIONS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ITSMARTSOLUTIONS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ITSMARTSOLUTIONS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ITSMARTSOLUTIONS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#Download necessary NLTK corpora:\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # <— this is the new one needed\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Move one level up from the notebooks folder\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Add /src to the Python path if not already there\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from llm_utils.llm_client import ask_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43fdd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Ask model to generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1fd6ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization in Large Language Models (LLMs) is a technique used to reduce the precision of the weights or activations within the model during training and inference. This process helps to decrease memory usage, accelerate computation, and sometimes even improve performance on certain tasks.\n",
      "\n",
      "Here’s a brief summary:\n",
      "\n",
      "1. **Reduction in Precision**: Quantization reduces the number of bits required to represent each weight or activation. Commonly used formats include 8-bit (int8), 4-bit (int4), and even lower precision like 2-bit (int2) for weights, while activations are often kept at higher precision.\n",
      "\n",
      "2. **Memory Efficiency**: By using fewer bits, the model requires less memory to store its parameters, making it more efficient both in terms of storage and during deployment on devices with limited resources.\n",
      "\n",
      "3. **Faster Computation**: Quantization can lead to faster computations since operations on lower-precision numbers are generally faster than those on higher-precision numbers (e.g., 8-bit integer operations are typically faster than 32-bit floating-point operations).\n",
      "\n",
      "4. **Performance Trade-offs**: While quantization offers benefits in terms of memory and computation, it can sometimes degrade model performance. However, recent advances have led to improved methods that mitigate this issue, often resulting in negligible or even no loss in accuracy for many tasks.\n",
      "\n",
      "5. **Implementation Challenges**: Quantization requires careful implementation to ensure that the model's performance does not suffer significantly. Techniques like dynamic quantization (quantizing during inference) and static quantization (quantizing during training) are used to balance precision with performance.\n",
      "\n",
      "Overall, quantization is a powerful tool in optimizing large language models for deployment on various hardware platforms while maintaining or improving their effectiveness.\n"
     ]
    }
   ],
   "source": [
    "# Basic usage\n",
    "answer = ask_model(\"Summarize what quantization does in LLMs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3278b08-c99d-452b-b6b5-26153763b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Save the result after answer is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb56ffb-7c93-4a08-a720-f92fa0f4d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model output for reuse by other notebooks or models\n",
    "os.makedirs(\"outputs/results\", exist_ok=True)\n",
    "\n",
    "with open(\"outputs/results/quantization_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask_model(f\"Make this answer simpler and shorter: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9364c0f8-6869-40e0-91f6-22f6e77071c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three main ideas in the provided text are:\n",
      "\n",
      "1. **Reduction in Memory Footprint**: Quantization reduces the precision of model weights from 32-bit floating-point numbers to lower bit depths (e.g., 16-bit, 8-bit, or 4-bit), significantly decreasing the memory required to store the model. This is particularly beneficial for efficient deployment on resource-constrained devices like mobile phones and edge computing devices.\n",
      "\n",
      "2. **Faster Inference**: Lower-precision operations typically require fewer computational resources, leading to faster inference times. This is advantageous in real-time applications where quick responses are crucial.\n",
      "\n",
      "3. **Impact on Performance**: While quantization improves efficiency, it can also degrade model performance. Techniques such as dynamic quantization and weight binarization are used to mitigate these effects while achieving significant efficiency gains. The impact of quantization varies depending on the specific model architecture and the degree of quantization applied.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The three main ideas in the provided text are:\\n\\n1. **Reduction in Memory Footprint**: Quantization reduces the precision of model weights from 32-bit floating-point numbers to lower bit depths (e.g., 16-bit, 8-bit, or 4-bit), significantly decreasing the memory required to store the model. This is particularly beneficial for efficient deployment on resource-constrained devices like mobile phones and edge computing devices.\\n\\n2. **Faster Inference**: Lower-precision operations typically require fewer computational resources, leading to faster inference times. This is advantageous in real-time applications where quick responses are crucial.\\n\\n3. **Impact on Performance**: While quantization improves efficiency, it can also degrade model performance. Techniques such as dynamic quantization and weight binarization are used to mitigate these effects while achieving significant efficiency gains. The impact of quantization varies depending on the specific model architecture and the degree of quantization applied.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_model(f\"What are the three main ideas in this text? {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f394ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Sentiment(polarity=0.11490929705215419, subjectivity=0.46014739229024954)\n",
      "Nouns: ['quantization', 'large language', 'llms', 'memory usage', 'accelerate computation', 'certain tasks', '’ s', 'brief summary', '* *', 'reduction', 'precision', '* *', 'quantization', 'commonly', '* *', 'memory efficiency', '* *', '* *', 'faster computation', '* *', 'quantization', 'lower-precision numbers', 'higher-precision numbers', '8-bit integer operations', '32-bit floating-point operations', '* *', 'performance trade-offs', '* *', 'degrade model performance', 'recent advances', '* * implementation', 'challenges', '* *', 'quantization', 'careful implementation', \"model 's performance\", 'techniques', 'dynamic quantization', 'static quantization', 'balance precision', 'overall', 'powerful tool', 'large language models', 'various hardware platforms']\n"
     ]
    }
   ],
   "source": [
    "#4.-------------------------------------------\n",
    "# Sentiment & keyword analysis using TextBlob\n",
    "#---------------------------------------------\n",
    "# TextBlob provides simple NLP tools built on top of NLTK.\n",
    "# Here, we analyze the model's answer (a string) to see:\n",
    "#   1) The overall sentiment (polarity and subjectivity)\n",
    "#   2) The key noun phrases, which highlight important concepts\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a TextBlob object from the model's output text\n",
    "blob = TextBlob(answer)\n",
    "\n",
    "# Sentiment gives two measures:\n",
    "#   polarity ∈ [-1, 1]   → negative to positive tone\n",
    "#   subjectivity ∈ [0, 1] → 0 = objective/factual, 1 = opinionated\n",
    "print(\"Sentiment:\", blob.sentiment)\n",
    "\n",
    "# noun_phrases extracts multi-word terms that act like \"keywords\"\n",
    "# (e.g. 'large language models', 'quantization', 'memory efficiency')\n",
    "# These show the main technical themes or entities discussed in the text.\n",
    "print(\"Nouns:\", blob.noun_phrases)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-llm-lab",
   "language": "python",
   "name": "notebook-llm-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
