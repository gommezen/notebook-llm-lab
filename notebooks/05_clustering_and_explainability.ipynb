{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# ğŸ“˜ Notebook: 05_clustering_and_explainability.ipynb\n",
    "#\n",
    "# Purpose:\n",
    "#   Discover patterns in runs via clustering and produce interpretable\n",
    "#   summaries of each cluster's characteristics.\n",
    "#\n",
    "# Inputs :\n",
    "#   - data/strava/processed/run_summary_features.parquet  (Stage 4 output)\n",
    "#\n",
    "# Outputs:\n",
    "#   - data/strava/processed/run_clusters.parquet\n",
    "#   - (optional) data/strava/processed/cluster_profiles.csv\n",
    "#\n",
    "# Steps:\n",
    "#   1) Load feature dataset and define feature matrix\n",
    "#   2) Quality checks: NaNs, scaling confirmation\n",
    "#   3) Model selection: find a reasonable k via silhouette\n",
    "#   4) Fit clustering (KMeans baseline)\n",
    "#   5) 2D projection (PCA) for visualization\n",
    "#   6) Cluster profiling (means, z-scores, rankings)\n",
    "#   7) Lightweight explainability proxy (tiny decision tree)\n",
    "#   8) Export artifacts (+ optional Neo4j write-back)\n",
    "#   9) Brief summary\n",
    "#\n",
    "# Notes:\n",
    "#   - Keep it simple and robust; add fancy variants (DBSCAN/HDBSCAN/UMAP) later.\n",
    "#   - All plots are sanity checks; interpret with domain sense (training context).\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Load dataset & pick features ----------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_path = Path(\"../data/strava/processed/run_summary_features.parquet\")\n",
    "df = pd.read_parquet(in_path)\n",
    "print(f\"âœ… Loaded {len(df):,} runs Ã— {len(df.columns)} columns\")\n",
    "\n",
    "# Candidate feature set (numeric, scaled in Stage 4).\n",
    "# If you changed names in Stage 4, update here to match.\n",
    "feature_cols = [\n",
    "    \"avg_pace\",\n",
    "    \"avg_cadence\",\n",
    "    \"elevation_gain\",\n",
    "    \"fatigue_index\",\n",
    "    \"pace_variability\",\n",
    "    \"elev_ratio\",\n",
    "]\n",
    "\n",
    "# Keep a copy of IDs/dates for later merging/exports\n",
    "meta_cols = [\"run_id\", \"date\", \"total_distance_km\", \"duration_min\", \"weekday\", \"month\"]\n",
    "available_meta = [c for c in meta_cols if c in df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Quality checks: missingness & scaling confirmation -------------------\n",
    "# Interpretation tip:\n",
    "# - Clustering is allergic to NaNs; either drop rows with NaNs in features\n",
    "#   or impute. Here we impute safely to keep all runs.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X_raw = df[feature_cols]\n",
    "missing_counts = X_raw.isna().sum().sort_values(ascending=False)\n",
    "print(\"NaNs per feature:\\n\", missing_counts.to_string())\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X = imputer.fit_transform(X_raw)\n",
    "\n",
    "# Optional quick scale check: If you DIDN'T scale in Stage 4, uncomment:\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "print(\"âœ… Feature matrix ready:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Model selection: silhouette sweep for k ------------------------------\n",
    "# Interpretation tip:\n",
    "# - Silhouette score (0..1) measures how well-separated clusters are.\n",
    "# - Look for an â€œelbowâ€ or local maximum across k values.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k_values = list(range(2, 10))\n",
    "scores = []\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "    labels = km.fit_predict(X)\n",
    "    s = silhouette_score(X, labels)\n",
    "    scores.append(s)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(k_values, scores, marker=\"o\")\n",
    "plt.title(\"Silhouette score vs. k\")\n",
    "plt.xlabel(\"k (number of clusters)\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[int(np.argmax(scores))]\n",
    "print(f\"ğŸ§­ Suggested k (by silhouette): {best_k}  |  scores={['{:.3f}'.format(v) for v in scores]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Interpretation: The silhouette curve peaks strongly at k = 2, indicating two clearly distinct run patterns. Beyond that, cluster separation deteriorates sharply, so further partitioning would fragment coherent patterns rather than reveal new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4) Fit baseline KMeans clustering --------------------------------------\n",
    "# Interpretation tip:\n",
    "# - This assigns each run to a cluster. We'll profile clusters next to\n",
    "#   understand their training â€œpersonalityâ€ (e.g., long hills vs recovery).\n",
    "k = best_k  # you can override manually after seeing the curve\n",
    "kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "df_clusters = df.copy()\n",
    "df_clusters[\"cluster\"] = labels\n",
    "print(df_clusters[\"cluster\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) 2D projection via PCA for visualization -----------------------------\n",
    "# Interpretation tip:\n",
    "# - This is just for visualization. Clusters that look separated in PCA space\n",
    "#   are typically distinct in multivariate space, but PCA flattens reality.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "proj = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "scatter = plt.scatter(proj[:,0], proj[:,1], c=labels, s=35, alpha=0.8, cmap=\"tab10\")\n",
    "plt.title(\"PCA projection colored by cluster\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.colorbar(scatter, label=\"cluster\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6) Cluster profiling: means, z-scores, and feature ranking -------------\n",
    "# Interpretation tip:\n",
    "# - Means: absolute values per feature per cluster.\n",
    "# - Z-scores: how far a cluster's mean sits from the global mean (std units).\n",
    "# - Ranking: which features most distinguish each cluster.\n",
    "\n",
    "# Compute cluster means (feature space)\n",
    "cluster_means = (\n",
    "    df_clusters.groupby(\"cluster\")[feature_cols]\n",
    "    .mean()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Global mean/std for z-score normalization\n",
    "global_mean = df[feature_cols].mean()\n",
    "global_std  = df[feature_cols].std(ddof=0).replace(0, 1.0)\n",
    "\n",
    "cluster_z = (cluster_means - global_mean) / global_std\n",
    "\n",
    "# Rank features within each cluster by absolute z-score (distinctiveness)\n",
    "ranked_features = (\n",
    "    cluster_z.abs()\n",
    "    .apply(lambda s: s.sort_values(ascending=False).index.tolist(), axis=1)\n",
    "    .to_frame(name=\"feature_rank\")\n",
    ")\n",
    "\n",
    "# Merge a compact profile table\n",
    "profile = cluster_means.round(3)\n",
    "profile[\"top_features\"] = ranked_features[\"feature_rank\"].apply(lambda xs: \", \".join(xs[:3]))\n",
    "\n",
    "print(\"ğŸ“Š Cluster means (selected):\")\n",
    "display(profile)\n",
    "\n",
    "# Optional: also show z-scores\n",
    "print(\"ğŸ“ˆ Cluster z-scores (distinctiveness):\")\n",
    "display(cluster_z.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7) Lightweight explainability proxy ------------------------------------\n",
    "# Interpretation tip:\n",
    "# - Train a tiny decision tree to predict cluster from features.\n",
    "# - The resulting feature importances are a proxy for what separates clusters.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X, labels)\n",
    "importances = pd.Series(tree.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "ax = importances.plot(kind=\"barh\", figsize=(6,4), title=\"Feature importance (Decision Tree)\")\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top separating features:\\n\", importances.round(3).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8) Export artifacts (+ optional Neo4j write-back) ----------------------\n",
    "out_parquet = Path(\"../data/strava/processed/run_clusters.parquet\")\n",
    "out_parquet.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Minimal, analysis-friendly export\n",
    "export_cols = available_meta + feature_cols + [\"cluster\"]\n",
    "df_clusters[export_cols].to_parquet(out_parquet, index=False)\n",
    "print(f\"âœ… Saved cluster assignments â†’ {out_parquet.resolve()}\")\n",
    "\n",
    "# Optional: write cluster IDs into Neo4j (uncomment to use)\n",
    "# from neo4j import GraphDatabase\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# NEO4J_URI  = os.getenv(\"NEO4J_URI\")\n",
    "# NEO4J_USER = os.getenv(\"NEO4J_USER\")\n",
    "# NEO4J_PASS = os.getenv(\"NEO4J_PASS\")\n",
    "# driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "# def set_cluster(tx, rid, cid):\n",
    "#     tx.run(\"MATCH (r:Run {run_id:$rid}) SET r.cluster=$cid\", rid=rid, cid=int(cid))\n",
    "# with driver.session() as s:\n",
    "#     for rid, cid in df_clusters[[\"run_id\",\"cluster\"]].itertuples(index=False):\n",
    "#         s.execute_write(set_cluster, rid, cid)\n",
    "# driver.close()\n",
    "# print(\"âœ… (Optional) Wrote cluster labels to Neo4j\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9) Brief summary --------------------------------------------------------\n",
    "n_runs = len(df_clusters)\n",
    "n_clusters = df_clusters[\"cluster\"].nunique()\n",
    "sizes = df_clusters[\"cluster\"].value_counts().sort_index().to_dict()\n",
    "\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"Stage 5 Summary\")\n",
    "print(f\"- Runs clustered         : {n_runs}\")\n",
    "print(f\"- Clusters discovered    : {n_clusters}\")\n",
    "print(f\"- Cluster sizes          : {sizes}\")\n",
    "print(\"- Top separating features: \")\n",
    "for f, v in pd.Series(tree.feature_importances_, index=feature_cols)\\\n",
    "            .sort_values(ascending=False).round(3).items():\n",
    "    if v > 0:\n",
    "        print(f\"   â€¢ {f}: {v}\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "# Interpretation pointers:\n",
    "# - Check 'Cluster means' and 'Cluster z-scores' to label clusters (e.g., 'Hilly long runs', 'Recovery spins').\n",
    "# - If silhouette is flat or low, try a different k, different features, or DBSCAN.\n",
    "# - For presentation, pair PCA plot with per-cluster histograms of pace/cadence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(profile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## ğŸ Stage 5 Summary â€” Clustering and Explainability Insights\n",
    "\n",
    "**Data coverage**  \n",
    "A total of **697 runs** were analyzed after preprocessing and feature engineering.  \n",
    "Each run was assigned to one of **two clusters**, the number suggested by the silhouette analysis (peak score â‰ˆ 0.66).\n",
    "\n",
    "**Cluster composition**  \n",
    "| Cluster | Runs | Approx. Share | Character |\n",
    "|:--------:|-----:|:--------------|:-----------|\n",
    "| 0 | 630 | ~90 % | Predominant, steady training pattern |\n",
    "| 1 | 67  | ~10 % | Distinct minority of high-intensity or terrain-specific runs |\n",
    "\n",
    "**Feature separation (Decision-Tree importance)**  \n",
    "- **avg_cadence (0.71)** â€” the primary discriminator.  \n",
    "  Higher cadence = faster, rhythmically tighter sessions; lower cadence = relaxed endurance runs.  \n",
    "- **elev_ratio (0.29)** â€” secondary factor describing hilliness or elevation variation per kilometre.\n",
    "\n",
    "**Interpretation of cluster behavior**  \n",
    "- **Cluster 0** likely represents *steady endurance or easy runs* â€” moderate cadence, smoother pacing, relatively uniform elevation.  \n",
    "- **Cluster 1** captures *performance or quality sessions* â€” higher cadence, greater variability, or steeper profiles (intervals, hills, or tempo work).\n",
    "\n",
    "**Overall insight**  \n",
    "Your training history naturally organizes into **two dominant performance regimes**:\n",
    "1. A large body of consistent aerobic runs forming the structural base of your training.  \n",
    "2. A smaller but distinctive set of faster or more intense efforts that drive adaptation.\n",
    "\n",
    "These empirical patterns align with coaching intuition â€” the data now provides quantitative confirmation.  \n",
    "Future stages (6 â†’ modeling, 7 â†’ dashboard/Neo4j integration) will build on this foundation to predict outcomes and visualize how intensity, cadence, and elevation interact over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-llm-lab",
   "language": "python",
   "name": "notebook-llm-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
