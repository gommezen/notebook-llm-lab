{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# üìò Notebook: 05_clustering_and_explainability.ipynb\n",
    "#\n",
    "# Purpose:\n",
    "#   Discover patterns in runs via clustering and produce interpretable\n",
    "#   summaries of each cluster's characteristics.\n",
    "#\n",
    "# Inputs :\n",
    "#   - data/strava/processed/run_summary_features.parquet  (Stage 4 output)\n",
    "#\n",
    "# Outputs:\n",
    "#   - data/strava/processed/run_clusters.parquet\n",
    "#   - (optional) data/strava/processed/cluster_profiles.csv\n",
    "#\n",
    "# Steps:\n",
    "#   1) Load feature dataset and define feature matrix\n",
    "#   2) Quality checks: NaNs, scaling confirmation\n",
    "#   3) Model selection: find a reasonable k via silhouette\n",
    "#   4) Fit clustering (KMeans baseline)\n",
    "#   5) 2D projection (PCA) for visualization\n",
    "#   6) Cluster profiling (means, z-scores, rankings)\n",
    "#   7) Lightweight explainability proxy (tiny decision tree)\n",
    "#   8) Export artifacts (+ optional Neo4j write-back)\n",
    "#   9) Brief summary\n",
    "#\n",
    "# Notes:\n",
    "#   - Keep it simple and robust; add fancy variants (DBSCAN/HDBSCAN/UMAP) later.\n",
    "#   - All plots are sanity checks; interpret with domain sense (training context).\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Load dataset & pick features ----------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_path = Path(\"../data/strava/processed/run_summary_features.parquet\")\n",
    "df = pd.read_parquet(in_path)\n",
    "print(f\"‚úÖ Loaded {len(df):,} runs √ó {len(df.columns)} columns\")\n",
    "\n",
    "# Candidate feature set (numeric, scaled in Stage 4).\n",
    "# If you changed names in Stage 4, update here to match.\n",
    "feature_cols = [\n",
    "    \"avg_pace\",\n",
    "    \"avg_cadence\",\n",
    "    \"elevation_gain\",\n",
    "    \"fatigue_index\",\n",
    "    \"pace_variability\",\n",
    "    \"elev_ratio\",\n",
    "]\n",
    "\n",
    "# Keep a copy of IDs/dates for later merging/exports\n",
    "meta_cols = [\"run_id\", \"date\", \"total_distance_km\", \"duration_min\", \"weekday\", \"month\"]\n",
    "available_meta = [c for c in meta_cols if c in df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Quality checks: missingness & scaling confirmation -------------------\n",
    "# Interpretation tip:\n",
    "# - Clustering is allergic to NaNs; either drop rows with NaNs in features\n",
    "#   or impute. Here we impute safely to keep all runs.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X_raw = df[feature_cols]\n",
    "missing_counts = X_raw.isna().sum().sort_values(ascending=False)\n",
    "print(\"NaNs per feature:\\n\", missing_counts.to_string())\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X = imputer.fit_transform(X_raw)\n",
    "\n",
    "# Optional quick scale check: If you DIDN'T scale in Stage 4, uncomment:\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "print(\"‚úÖ Feature matrix ready:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Model selection: silhouette sweep for k ------------------------------\n",
    "# Interpretation tip:\n",
    "# - Silhouette score (0..1) measures how well-separated clusters are.\n",
    "# - Look for an ‚Äúelbow‚Äù or local maximum across k values.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k_values = list(range(2, 10))\n",
    "scores = []\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "    labels = km.fit_predict(X)\n",
    "    s = silhouette_score(X, labels)\n",
    "    scores.append(s)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(k_values, scores, marker=\"o\")\n",
    "plt.title(\"Silhouette score vs. k\")\n",
    "plt.xlabel(\"k (number of clusters)\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[int(np.argmax(scores))]\n",
    "print(f\"üß≠ Suggested k (by silhouette): {best_k}  |  scores={['{:.3f}'.format(v) for v in scores]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Interpretation: The silhouette curve peaks strongly at k = 2, indicating two clearly distinct run patterns. Beyond that, cluster separation deteriorates sharply, so further partitioning would fragment coherent patterns rather than reveal new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4) Fit baseline KMeans clustering --------------------------------------\n",
    "# Interpretation tip:\n",
    "# - This assigns each run to a cluster. We'll profile clusters next to\n",
    "#   understand their training ‚Äúpersonality‚Äù (e.g., long hills vs recovery).\n",
    "k = best_k  # you can override manually after seeing the curve\n",
    "kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "df_clusters = df.copy()\n",
    "df_clusters[\"cluster\"] = labels\n",
    "print(df_clusters[\"cluster\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) 2D projection via PCA for visualization -----------------------------\n",
    "# Interpretation tip:\n",
    "# - This is just for visualization. Clusters that look separated in PCA space\n",
    "#   are typically distinct in multivariate space, but PCA flattens reality.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "proj = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "scatter = plt.scatter(proj[:,0], proj[:,1], c=labels, s=35, alpha=0.8, cmap=\"tab10\")\n",
    "plt.title(\"PCA projection colored by cluster\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.colorbar(scatter, label=\"cluster\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6) Cluster profiling: means, z-scores, and feature ranking -------------\n",
    "# Interpretation tip:\n",
    "# - Means: absolute values per feature per cluster.\n",
    "# - Z-scores: how far a cluster's mean sits from the global mean (std units).\n",
    "# - Ranking: which features most distinguish each cluster.\n",
    "\n",
    "# Compute cluster means (feature space)\n",
    "cluster_means = (\n",
    "    df_clusters.groupby(\"cluster\")[feature_cols]\n",
    "    .mean()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Global mean/std for z-score normalization\n",
    "global_mean = df[feature_cols].mean()\n",
    "global_std  = df[feature_cols].std(ddof=0).replace(0, 1.0)\n",
    "\n",
    "cluster_z = (cluster_means - global_mean) / global_std\n",
    "\n",
    "# Rank features within each cluster by absolute z-score (distinctiveness)\n",
    "ranked_features = (\n",
    "    cluster_z.abs()\n",
    "    .apply(lambda s: s.sort_values(ascending=False).index.tolist(), axis=1)\n",
    "    .to_frame(name=\"feature_rank\")\n",
    ")\n",
    "\n",
    "# Merge a compact profile table\n",
    "profile = cluster_means.round(3)\n",
    "profile[\"top_features\"] = ranked_features[\"feature_rank\"].apply(lambda xs: \", \".join(xs[:3]))\n",
    "\n",
    "print(\"üìä Cluster means (selected):\")\n",
    "display(profile)\n",
    "\n",
    "# Optional: also show z-scores\n",
    "print(\"üìà Cluster z-scores (distinctiveness):\")\n",
    "display(cluster_z.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7) Lightweight explainability proxy ------------------------------------\n",
    "# Interpretation tip:\n",
    "# - Train a tiny decision tree to predict cluster from features.\n",
    "# - The resulting feature importances are a proxy for what separates clusters.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X, labels)\n",
    "importances = pd.Series(tree.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "ax = importances.plot(kind=\"barh\", figsize=(6,4), title=\"Feature importance (Decision Tree)\")\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top separating features:\\n\", importances.round(3).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8) Export artifacts (+ optional Neo4j write-back) ----------------------\n",
    "out_parquet = Path(\"../data/strava/processed/run_clusters.parquet\")\n",
    "out_parquet.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Minimal, analysis-friendly export\n",
    "export_cols = available_meta + feature_cols + [\"cluster\"]\n",
    "df_clusters[export_cols].to_parquet(out_parquet, index=False)\n",
    "print(f\"‚úÖ Saved cluster assignments ‚Üí {out_parquet.resolve()}\")\n",
    "\n",
    "# Optional: write cluster IDs into Neo4j (uncomment to use)\n",
    "# from neo4j import GraphDatabase\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# NEO4J_URI  = os.getenv(\"NEO4J_URI\")\n",
    "# NEO4J_USER = os.getenv(\"NEO4J_USER\")\n",
    "# NEO4J_PASS = os.getenv(\"NEO4J_PASS\")\n",
    "# driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "# def set_cluster(tx, rid, cid):\n",
    "#     tx.run(\"MATCH (r:Run {run_id:$rid}) SET r.cluster=$cid\", rid=rid, cid=int(cid))\n",
    "# with driver.session() as s:\n",
    "#     for rid, cid in df_clusters[[\"run_id\",\"cluster\"]].itertuples(index=False):\n",
    "#         s.execute_write(set_cluster, rid, cid)\n",
    "# driver.close()\n",
    "# print(\"‚úÖ (Optional) Wrote cluster labels to Neo4j\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9) Brief summary --------------------------------------------------------\n",
    "n_runs = len(df_clusters)\n",
    "n_clusters = df_clusters[\"cluster\"].nunique()\n",
    "sizes = df_clusters[\"cluster\"].value_counts().sort_index().to_dict()\n",
    "\n",
    "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(\"Stage 5 Summary\")\n",
    "print(f\"- Runs clustered         : {n_runs}\")\n",
    "print(f\"- Clusters discovered    : {n_clusters}\")\n",
    "print(f\"- Cluster sizes          : {sizes}\")\n",
    "print(\"- Top separating features: \")\n",
    "for f, v in pd.Series(tree.feature_importances_, index=feature_cols)\\\n",
    "            .sort_values(ascending=False).round(3).items():\n",
    "    if v > 0:\n",
    "        print(f\"   ‚Ä¢ {f}: {v}\")\n",
    "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "\n",
    "# Interpretation pointers:\n",
    "# - Check 'Cluster means' and 'Cluster z-scores' to label clusters (e.g., 'Hilly long runs', 'Recovery spins').\n",
    "# - If silhouette is flat or low, try a different k, different features, or DBSCAN.\n",
    "# - For presentation, pair PCA plot with per-cluster histograms of pace/cadence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(profile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## üèÅ Stage 5 Summary ‚Äî Clustering and Explainability Insights\n",
    "\n",
    "**Data coverage**  \n",
    "A total of **697 runs** were analyzed after preprocessing and feature engineering.  \n",
    "Each run was assigned to one of **two clusters**, the number suggested by the silhouette analysis (peak score ‚âà 0.66).\n",
    "\n",
    "**Cluster composition**  \n",
    "| Cluster | Runs | Approx. Share | Character |\n",
    "|:--------:|-----:|:--------------|:-----------|\n",
    "| 0 | 630 | ~90 % | Predominant, steady training pattern |\n",
    "| 1 | 67  | ~10 % | Distinct minority of high-intensity or terrain-specific runs |\n",
    "\n",
    "**Feature separation (Decision-Tree importance)**  \n",
    "- **avg_cadence (0.71)** ‚Äî the primary discriminator.  \n",
    "  Higher cadence = faster, rhythmically tighter sessions; lower cadence = relaxed endurance runs.  \n",
    "- **elev_ratio (0.29)** ‚Äî secondary factor describing hilliness or elevation variation per kilometre.\n",
    "\n",
    "**Interpretation of cluster behavior**  \n",
    "- **Cluster 0** likely represents *steady endurance or easy runs* ‚Äî moderate cadence, smoother pacing, relatively uniform elevation.  \n",
    "- **Cluster 1** captures *performance or quality sessions* ‚Äî higher cadence, greater variability, or steeper profiles (intervals, hills, or tempo work).\n",
    "\n",
    "**Overall insight**  \n",
    "Your training history naturally organizes into **two dominant performance regimes**:\n",
    "1. A large body of consistent aerobic runs forming the structural base of your training.  \n",
    "2. A smaller but distinctive set of faster or more intense efforts that drive adaptation.\n",
    "\n",
    "These empirical patterns align with coaching intuition ‚Äî the data now provides quantitative confirmation.  \n",
    "Future stages (6 ‚Üí modeling, 7 ‚Üí dashboard/Neo4j integration) will build on this foundation to predict outcomes and visualize how intensity, cadence, and elevation interact over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-llm-lab",
   "language": "python",
   "name": "notebook-llm-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
